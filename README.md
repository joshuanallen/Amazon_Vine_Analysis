# Amazon_Vine_Analysis
Amazon reviews NLP analysis

## Purpose
The purpose of this analysis is to determine if the Amazon Vine pay-to-review program generates a bias favorable for those products being reviewed.

## Resources
Data: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz
Software: PySpark 3.0.3, Amazon RDS, PGAdmin4, Google Colaboratory

## Analysis

### Process
Using PySpark 3.0.3 on Google Colab, we imported the [Lawn and Garden reviews dataset](https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz) from a [list of review datasets](https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt) and then transformed the data for analysis. During the transformation stage, we generated four tables that were then exported to PGAdmin using an RDS server. The four exported tables are shown below in Pictures 1.1 - 1.4.

**Picture 1.1: Customers table includes the `customer_id` and a count of the number of reviews they've given**

![Customers table](https://github.com/joshuanallen/Amazon_Vine_Analysis/blob/10a55b10976b9df1c278d90a2b861ebe3b7962c9/tables/customers_SQL_data.png)


**Picture 1.2: Products table includes the `product_id` and `product_title`**

![Products table](https://github.com/joshuanallen/Amazon_Vine_Analysis/blob/10a55b10976b9df1c278d90a2b861ebe3b7962c9/tables/products_SQL_table.png)


**Picture 1.3: Review id table includes the `review_id`, `customer_id`, `product_id`, `product_parent`, and `review_date`**

![Review id table](https://github.com/joshuanallen/Amazon_Vine_Analysis/blob/10a55b10976b9df1c278d90a2b861ebe3b7962c9/tables/review_id_SQL_table.png)


**Picture 1.4: Vine Table includes `review_id`, `star_rating`, `helpful_votes`, `total_votes`, vine program participation, and verified purchase from reviewer**
![Vine Table](https://github.com/joshuanallen/Amazon_Vine_Analysis/blob/10a55b10976b9df1c278d90a2b861ebe3b7962c9/tables/vine_SQL_table.png)


The vine table was then used to identify bias in the vine reviewers submissions compared to non-vine reviewers submissions by identifying the amount of 5-star (perfect) ratings as a percentage of the total votes.

### Results
1. Total Reviews:
    - Vine participants: 373
    - Non-Vine participants: 45,706
2. 5-Star Reviews:
    - Vine participants: 171
    - Non-Vine participants: 22,546
3. Percentage of 5-Star Reviews:
    - Vine participants: 45.84%
    - Non-Vine participants: 49.33%

Because the Vine participant 5-star review percentage (46%) is lower than the non-Vine review percentage (49%), we can conclude there is seemingly no additional favorable bias towards the products amongst Vine reviewers when compared to non-Vine reviewers.

### Removing reviews from non-verified purchasers from non-Vine participants
1. Total Reviews:
    - Verified Non-Vine participants: 30,577
2. 5-Star Reviews:
    - Verified Non-Vine participants: 15,908
3. Percentage of 5-Star Reviews:
    - Verified Non-Vine participants: 52.03%

Removing the non-verified purchaser reviews shows higher 5-star ratings by percentage for non-Vine program products, therefore this also supports the idea of Vine reviews not being bias in favor of a product. The percentage of reviews receiving 5 stars for Vine products (46%) is lower than verified purchasers of non-Vine products (52%).

### Additional analysis to continue study
1. A meta-analysis of *all* the datasets provided by category may give a better overall picture of the bias in the program as having a larger samepl size across multiple categories would limit any bias between categories.

2. Because rating systems are notoriously polarizing, we can eliminate the 1 and 5 star ratings and compare the outcomes using only the 2-4 star ratings.

The data from non-paid reviews can be more polarizing, which would include more 1s and 5s because they are self motivated to provide a review due to a good or bad experience. This may skew the data away from the nuance of a 5 point ratings system.

This differs from "paid reviews" in the Vine program as they are motivated more to provide more nuanced feedback as their feedback is not generated by a good or bad experience.

To account for the response bias between the two groups we should plot the responses to identify if this 1 and 5 star polarization exists in the data and is pervasive across additional datasets.

3. Use NLP analysis to identify and remove false reviews, fake or duplicated reviews, and reviews that may apply to shipping or delivery issues and are not reflective on the product themselves.

### Limitations of current dataset
The data is limited to Lawn and Garden products, so additional datasets under different product categories may yield different results.

